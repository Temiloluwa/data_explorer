{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def guess_tribe(name: str) -> str:\n",
    "    \"\"\"Guesses your tribe based on your name\n",
    "\n",
    "    Args:\n",
    "        name: your name\n",
    "    \"\"\"\n",
    "    if \"A\" < name[0]  < \"M\":\n",
    "        return \"Zulu\"\n",
    "    \n",
    "    elif \"M\" <= name[0] < \"R\":\n",
    "        return \"Yoruba\"\n",
    "    else:\n",
    "        return \"Isoko\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "model_with_tools = model.bind_tools([guess_tribe])\n",
    "\n",
    "response = model_with_tools.invoke(\"My friends are Chloe, Ryan, James and Quentin, could you tell me their heritage?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refusal': None,\n",
      " 'tool_calls': [{'function': {'arguments': '{\"name\": \"Chloe\"}',\n",
      "                              'name': 'guess_tribe'},\n",
      "                 'id': 'call_G4W45cKuTUPT6awzw1S9ezfF',\n",
      "                 'type': 'function'},\n",
      "                {'function': {'arguments': '{\"name\": \"Ryan\"}',\n",
      "                              'name': 'guess_tribe'},\n",
      "                 'id': 'call_gBP7ZKtqhEcz9yuNiL58fVDy',\n",
      "                 'type': 'function'},\n",
      "                {'function': {'arguments': '{\"name\": \"James\"}',\n",
      "                              'name': 'guess_tribe'},\n",
      "                 'id': 'call_4FnDOhwo9yDGXg2WXgHF0nYY',\n",
      "                 'type': 'function'},\n",
      "                {'function': {'arguments': '{\"name\": \"Quentin\"}',\n",
      "                              'name': 'guess_tribe'},\n",
      "                 'id': 'call_aCZMRwt1cusSh0EQRG1sN6WO',\n",
      "                 'type': 'function'}]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(response.additional_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'guess_tribe',\n",
       "  'args': {'name': 'Chloe'},\n",
       "  'id': 'call_G4W45cKuTUPT6awzw1S9ezfF',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'guess_tribe',\n",
       "  'args': {'name': 'Ryan'},\n",
       "  'id': 'call_gBP7ZKtqhEcz9yuNiL58fVDy',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'guess_tribe',\n",
       "  'args': {'name': 'James'},\n",
       "  'id': 'call_4FnDOhwo9yDGXg2WXgHF0nYY',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'guess_tribe',\n",
       "  'args': {'name': 'Quentin'},\n",
       "  'id': 'call_aCZMRwt1cusSh0EQRG1sN6WO',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm_with_tools = model.bind_tools([multiply])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_XLI6Vla4MLH8aobHwzHoyG0n', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 54, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-26e34d30-3134-4f82-afcc-f8012de6a541-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_XLI6Vla4MLH8aobHwzHoyG0n', 'type': 'tool_call'}], usage_metadata={'input_tokens': 54, 'output_tokens': 18, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"
     ]
    }
   ],
   "source": [
    "result_a = llm_with_tools.invoke(\"What is 2 multiplied by 3?\")\n",
    "pprint(result_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'args': {'a': 2, 'b': 3},\n",
      "  'id': 'call_XLI6Vla4MLH8aobHwzHoyG0n',\n",
      "  'name': 'multiply',\n",
      "  'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(result_a.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='6', name='multiply', tool_call_id='call_XLI6Vla4MLH8aobHwzHoyG0n')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply.invoke(result_a.tool_calls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unsupported message type: <class 'list'>\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm_with_tools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mresult_a\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtool_calls\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codes-and-scripts/projects/data_explorer/.research-env/lib/python3.12/site-packages/langchain_core/runnables/base.py:5352\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5347\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5348\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5349\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5350\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5351\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5353\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5354\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codes-and-scripts/projects/data_explorer/.research-env/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:287\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m--> 287\u001b[0m             [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[1;32m    288\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    289\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    290\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    291\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    292\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    293\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    294\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    295\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/codes-and-scripts/projects/data_explorer/.research-env/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:267\u001b[0m, in \u001b[0;36mBaseChatModel._convert_input\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StringPromptValue(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, Sequence):\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39m\u001b[43mconvert_to_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m     )\n",
      "File \u001b[0;32m~/codes-and-scripts/projects/data_explorer/.research-env/lib/python3.12/site-packages/langchain_core/messages/utils.py:357\u001b[0m, in \u001b[0;36mconvert_to_messages\u001b[0;34m(messages)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(messages, PromptValue):\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m messages\u001b[38;5;241m.\u001b[39mto_messages()\n\u001b[0;32m--> 357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_convert_to_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "File \u001b[0;32m~/codes-and-scripts/projects/data_explorer/.research-env/lib/python3.12/site-packages/langchain_core/messages/utils.py:336\u001b[0m, in \u001b[0;36m_convert_to_message\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    334\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported message type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m     msg \u001b[38;5;241m=\u001b[39m create_message(message\u001b[38;5;241m=\u001b[39mmsg, error_code\u001b[38;5;241m=\u001b[39mErrorCode\u001b[38;5;241m.\u001b[39mMESSAGE_COERCION_FAILURE)\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _message\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Unsupported message type: <class 'list'>\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE "
     ]
    }
   ],
   "source": [
    "llm_with_tools.invoke([result_a.tool_calls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Step 1: Define your tools\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = {\n",
    "    \"add\": add,\n",
    "    \"multiply\": multiply\n",
    "}\n",
    "\n",
    "tools_list = list(tools.values())\n",
    "\n",
    "# Step 2: Initialize your language model\n",
    "llm = model\n",
    "\n",
    "# Step 3: Bind tools to the model\n",
    "agent = llm.bind_tools(tools_list)\n",
    "\n",
    "# Step 4: Define a function to process user queries\n",
    "def process_query(user_query: str) -> str:\n",
    "    # Invoke the agent with the user query\n",
    "    response = agent.invoke(user_query)\n",
    "    \n",
    "    # Check if the response contains a tool call\n",
    "    if response.tool_calls:\n",
    "        tool_msgs = [HumanMessage(user_query),\n",
    "                     response\n",
    "                     ]\n",
    "        for tool_call in response.tool_calls:\n",
    "            # Extract tool name and arguments\n",
    "            tool_name = tool_call[\"name\"]\n",
    "\n",
    "            # Invoke tools and append tool message\n",
    "            tool_msgs.append(tools[tool_name].invoke(tool_call))\n",
    "    \n",
    "    \n",
    "    # Invoke llm with Query and tool responses\n",
    "    final_response = agent.invoke(tool_msgs)\n",
    "    \n",
    "    return final_response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='What is the result of adding 3 and 4?', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_5ACf41h2py4MCcQ7Rr0jTuzJ', 'function': {'arguments': '{\"a\":3,\"b\":4}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 79, 'total_tokens': 97, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-85a673e3-7952-44d8-8538-4735c7bc9005-0', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 4}, 'id': 'call_5ACf41h2py4MCcQ7Rr0jTuzJ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 79, 'output_tokens': 18, 'total_tokens': 97, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='7', name='add', tool_call_id='call_5ACf41h2py4MCcQ7Rr0jTuzJ')]\n",
      "content='The result of adding 3 and 4 is 7.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 104, 'total_tokens': 119, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-c9c24060-ec5b-4c07-bb8c-3684b916e3f2-0' usage_metadata={'input_tokens': 104, 'output_tokens': 15, 'total_tokens': 119, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_query = \"What is the result of adding 3 and 4?\"\n",
    "final_response = process_query(user_query)\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='7', name='add', tool_call_id='call_RUVmH5OihmDkfBN7DDtuxkR8')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call = response.tool_calls[0]\n",
    "tool_name = tool_call[\"name\"]\n",
    "tool_args = tool_call[\"args\"]\n",
    "\n",
    "tools[tool_name].invoke(tool_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'add',\n",
       " 'args': {'a': 3, 'b': 4},\n",
       " 'id': 'call_RUVmH5OihmDkfBN7DDtuxkR8',\n",
       " 'type': 'tool_call'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'add',\n",
       "  'args': {'a': 3, 'b': 4},\n",
       "  'id': 'call_RUVmH5OihmDkfBN7DDtuxkR8',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_RUVmH5OihmDkfBN7DDtuxkR8', 'function': {'arguments': '{\"a\":3,\"b\":4}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 79, 'total_tokens': 97, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d6b94590-96b2-4641-95a6-022e4c626e43-0', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 4}, 'id': 'call_RUVmH5OihmDkfBN7DDtuxkR8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 79, 'output_tokens': 18, 'total_tokens': 97, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from typing import List\n",
    "\n",
    "class ToolAgent:\n",
    "    def __init__(self, model, tools: List[BaseTool]):\n",
    "        \"\"\"\n",
    "        Initialize agent with LLM and list of tools\n",
    "        \n",
    "        Args:\n",
    "            model: Language model instance\n",
    "            tools: List of tools (must be @tool decorated or BaseTool instances)\n",
    "        \"\"\"\n",
    "        self.llm = model\n",
    "        self.tools = {tool.name: tool for tool in tools}\n",
    "        self.agent = self.llm.bind_tools(tools)\n",
    "\n",
    "    def invoke(self, user_query: str) -> str:\n",
    "        \"\"\"\n",
    "        Process user query using available tools\n",
    "        \n",
    "        Args:\n",
    "            user_query: Input question/request from user\n",
    "            \n",
    "        Returns:\n",
    "            Final response after tool processing\n",
    "        \"\"\"\n",
    "        # Initial agent response\n",
    "        response = self.agent.invoke(user_query)\n",
    "        \n",
    "        # If no tool calls needed, return directly\n",
    "        if not response.tool_calls:\n",
    "            return response.content\n",
    "\n",
    "        # Prepare message history with original query and agent response\n",
    "        messages = [\n",
    "            HumanMessage(content=user_query),\n",
    "            response\n",
    "        ]\n",
    "\n",
    "        # Process each tool call\n",
    "        for tool_call in response.tool_calls:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            \n",
    "            if tool_name not in self.tools:\n",
    "                raise ValueError(f\"Unknown tool: {tool_name}\")\n",
    "\n",
    "            # Get the tool instance\n",
    "            tool = self.tools[tool_name]\n",
    "            \n",
    "            # Invoke the tool with the ENTIRE tool_call dictionary\n",
    "            tool_result = tool.invoke(tool_call)\n",
    "            \n",
    "            # Create proper ToolMessage with call ID and result\n",
    "            messages.append(tool_result)\n",
    "\n",
    "        # Get final response with tool outputs\n",
    "        self.messages = messages\n",
    "        final_response = self.agent.invoke(messages)\n",
    "        return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_agent = ToolAgent(\n",
    "    model=model,\n",
    "    tools=tools_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tool_agent.invoke(\"What would be 54 mulitplied by 26 plus 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='What would be 54 mulitplied by 26 plus 2', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_utU03Kxl0dEbQqucQchLwp90', 'function': {'arguments': '{\"a\": 54, \"b\": 26}', 'name': 'multiply'}, 'type': 'function'}, {'id': 'call_UDR4zhuI3ZjxKfsiHMBFotPk', 'function': {'arguments': '{\"a\": 54, \"b\": 2}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 81, 'total_tokens': 132, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-b681587a-5eae-481a-9614-802e1cd72789-0', tool_calls=[{'name': 'multiply', 'args': {'a': 54, 'b': 26}, 'id': 'call_utU03Kxl0dEbQqucQchLwp90', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 54, 'b': 2}, 'id': 'call_UDR4zhuI3ZjxKfsiHMBFotPk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 81, 'output_tokens': 51, 'total_tokens': 132, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
      " ToolMessage(content='1404', name='multiply', tool_call_id='call_utU03Kxl0dEbQqucQchLwp90'),\n",
      " ToolMessage(content='56', name='add', tool_call_id='call_UDR4zhuI3ZjxKfsiHMBFotPk')]\n"
     ]
    }
   ],
   "source": [
    "pprint(tool_agent.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".research-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
